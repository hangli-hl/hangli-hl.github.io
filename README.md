# Hang Li's Homepage

I am head of Research at Bytedance Technology. I joined Bytedance in Beijing in 2017.  I worked at the Research Laboratories of NEC Corporation during 1990 and 2001, and Microsoft Research Asia during 2001 and 2012, Naoh's Ark Lab of Huawei Technologies during 2012 and 2017.

I obtained a B.S. in Electrical and Electronics Engineering from Kyoto University in 1988 and a M.S. in Electrical and Electronics Engineering from Kyoto University in 1990. I earned my Ph.D. in Computer Science from the University of Tokyo in 1998.

I am an ACM Fellow, ACL Fellow, and IEEE Fellow. My research areas include natural language processing, information retrieval, machine learning, and data mining.

### Contact

Bytedance Technology,
Fangheng,  No. 27,  North 3rd Ring West Road, Haidian District, Beijing, 100006, China

Mail: lihang.lh @ bytedance.com

### Selected Recent Papers

* Y Liu, P Sun, H Li, Large Language Models as Agents in Two-Player Games, arXiv:2402.08078, 2024.

* T Luong, X Zhang, Z Jie, P Sun, X Jin, H Li, Reft: Reasoning with Reinforced Fine-Tuning, arXiv preprint arXiv:2401.08967, 2024.

* Y Zeng, G Wei, J Zheng, J Zou, Y Wei, Y Zhang, H Li, Make Pixels Dance: High-dynamic video generation, CVPR 2024.

* H Wu, Y Jing, C Cheang, G Chen, J Xu, X Li, M Liu, H Li, T Kong, Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation, ICLR 2024.

* X Li, M Liu, H Zhang, C Yu, J Xu, H Wu, C Cheang, Y Jing, W Zhang, et al,  Vision-Language Foundation Models as Effective Robot Imitators, ICLR 2024.

* Y Liu, Y Yao, J Ton, X Zhang, R Cheng, Y Klochkov, M Taufiq, H Li, Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment, arXiv:2308.05374, 2023.

* Y Zeng, X Zhang, H Li, J Wang, J Zhang, W Zhou,  X2-VLM: All-In-One Pre-trained Model For Vision-Language Tasks, IEEE PAMI, 2023.

* H Li, Language Models: Past, Present, and Future, Communications of the ACM 65 (7), 56-63, 2023.

* Z Liu, Z Wang, Y Lin, H Li, A Neural-Symbolic Approach to Natural Language Understanding, EMNLP 2023 Finding.
  
* F Huang, H Zhou, Y Liu, H Li, M Huang, Directed Acyclic Transformer for Non-autoregressive Machine Translation, ICML 2022.

* Y Zeng, X Zhang, H Li, Multi-Grained Vision Language Pre-training: Aligning Texts with Visual Concepts, ICML 2022.

* Y Feng, Y Wang, H Li, A Sequence-to-Sequence Approach to Dialogue State Tracking, ACL 2021.

* S Zhang, H Huang, J Liu, H Li, Spelling Error Correction with Soft-Masked BERT, ACL 2020.

* X Zhang, H Xie, H Li, J CS Lui, Conversational Contextual Bandit: Algorithm and Application, WebConf 2020.

* Ziniu Hu, Yang Wang, Qu Peng, Hang Li, Unbiased LambdaMART: An Unbiased Pairwise Learning to Rank Algorithm, WebConf 2019. 

* Zichao Li, Xin Jiang, Lifeng Shang, Hang Li, Paraphrase Generation Using Deep Reinforcement Learning, EMNLP 2018.

### Recent Books

* Hang Li, Machine Learning Methods, translated by L Lin, H Zeng, Springer, 2024.
  
* 李航，机器学习方法，清华大学出版社，2022.
